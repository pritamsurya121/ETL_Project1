{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set spark environments\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/cloudera/parcels/Anaconda/bin/python\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/java/jdk1.8.0_161/jre\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/cloudera/parcels/SPARK2-2.3.0.cloudera2-1.cdh5.13.3.p0.316101/lib/spark2/\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.6-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-10-0-0-135.ec2.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.cloudera2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ETL_Project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f24440c2790>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('ETL_Project').master(\"local\").enableHiveSupport().getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe from the csv file and infering the schema\n",
    "df1 = spark.read.csv(\"SRC_ATM_TRANS\",  inferSchema=True, header=False )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: integer (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: integer (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: integer (nullable = true)\n",
      " |-- _c11: integer (nullable = true)\n",
      " |-- _c12: double (nullable = true)\n",
      " |-- _c13: double (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: integer (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: integer (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      " |-- _c20: double (nullable = true)\n",
      " |-- _c21: double (nullable = true)\n",
      " |-- _c22: integer (nullable = true)\n",
      " |-- _c23: string (nullable = true)\n",
      " |-- _c24: double (nullable = true)\n",
      " |-- _c25: integer (nullable = true)\n",
      " |-- _c26: integer (nullable = true)\n",
      " |-- _c27: integer (nullable = true)\n",
      " |-- _c28: integer (nullable = true)\n",
      " |-- _c29: double (nullable = true)\n",
      " |-- _c30: integer (nullable = true)\n",
      " |-- _c31: integer (nullable = true)\n",
      " |-- _c32: string (nullable = true)\n",
      " |-- _c33: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing Schema\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+------+---+--------+---+---+--------+-----------+----+----+------+------+----+----------+----+----------+----+----+------+------+-------+------------+------+----+----+----+----+-----+----+----+----+----------+\n",
      "| _c0|    _c1|_c2|   _c3|_c4|     _c5|_c6|_c7|     _c8|        _c9|_c10|_c11|  _c12|  _c13|_c14|      _c15|_c16|      _c17|_c18|_c19|  _c20|  _c21|   _c22|        _c23|  _c24|_c25|_c26|_c27|_c28| _c29|_c30|_c31|_c32|      _c33|\n",
      "+----+-------+---+------+---+--------+---+---+--------+-----------+----+----+------+------+----+----------+----+----------+----+----+------+------+-------+------------+------+----+----+----+----+-----+----+----+----+----------+\n",
      "|2017|January|  1|Sunday|  0|  Active|  1|NCR|NÃ¦stved|Farimagsvej|   8|4700|55.233|11.763| DKK|MasterCard|5643|Withdrawal|null|null| 55.23|11.761|2616038|    Naestved|281.15|1014|  87|   7| 260|0.215|  92| 500|Rain|light rain|\n",
      "|2017|January|  1|Sunday|  0|Inactive|  2|NCR|Vejgaard| Hadsundvej|  20|9000|57.043|  9.95| DKK|MasterCard|1764|Withdrawal|null|null|57.048| 9.935|2616235|NÃ¸rresundby|280.64|1020|  93|   9| 250| 0.59|  92| 500|Rain|light rain|\n",
      "+----+-------+---+------+---+--------+---+---+--------+-----------+----+----+------+------+----+----------+----+----------+----+----+------+------+-------+------------+------+----+----+----+----+-----+----+----+----+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showing the elements of the dataframe\n",
    "df1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying the schema instead of inferring it \n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, BooleanType, DoubleType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileSchema = StructType([StructField('year', IntegerType(),True),\n",
    "                        StructField('month', StringType(),True),\n",
    "                        StructField('day', IntegerType(),True),\n",
    "                        StructField('weekday', StringType(),True),\n",
    "                        StructField('hour', IntegerType(),True),\n",
    "                        StructField('atm_status', StringType(),True),\n",
    "                        StructField('atm_id', StringType(),True),\n",
    "                        StructField('atm_manufacturer', StringType(),True),\n",
    "                        StructField('atm_location', StringType(),True),\n",
    "                        StructField('atm_streetname', StringType(),True),\n",
    "                        StructField('atm_street_number', IntegerType(),True),\n",
    "                        StructField('atm_zipcode', IntegerType(),True),\n",
    "                        StructField('atm_lat', FloatType(),True),\n",
    "                        StructField('atm_lon', FloatType(),True),\n",
    "                        StructField('currency', StringType(),True),\n",
    "                        StructField('card_type', StringType(),True),\n",
    "                        StructField('transaction_amount', IntegerType(),True),\n",
    "                        StructField('service', StringType(),True), \n",
    "                        StructField('message_code', StringType(),True),\n",
    "                        StructField('message_text', StringType(),True),\n",
    "                        StructField('weather_lat', FloatType(),True),\n",
    "                        StructField('weather_lon', FloatType(),True),\n",
    "                        StructField('weather_city_id', IntegerType(),True),\n",
    "                        StructField('weather_city_name', StringType(),True),\n",
    "                        StructField('temp', FloatType(),True), \n",
    "                        StructField('pressure', IntegerType(),True),\n",
    "                        StructField('humidity', IntegerType(),True),\n",
    "                        StructField('wind_speed', IntegerType(),True),\n",
    "                        StructField('wind_deg', IntegerType(),True),\n",
    "                        StructField('rain_3h', FloatType(),True),\n",
    "                        StructField('clouds_all', IntegerType(),True),\n",
    "                        StructField('weather_id', IntegerType(),True),\n",
    "                        StructField('weather_main', StringType(),True),\n",
    "                        StructField('weather_description', StringType(),True),\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe from the csv file and infering the schema\n",
    "df = spark.read.csv(\"SRC_ATM_TRANS\", header = False, schema = fileSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- atm_status: string (nullable = true)\n",
      " |-- atm_id: string (nullable = true)\n",
      " |-- atm_manufacturer: string (nullable = true)\n",
      " |-- atm_location: string (nullable = true)\n",
      " |-- atm_streetname: string (nullable = true)\n",
      " |-- atm_street_number: integer (nullable = true)\n",
      " |-- atm_zipcode: integer (nullable = true)\n",
      " |-- atm_lat: float (nullable = true)\n",
      " |-- atm_lon: float (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- transaction_amount: integer (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- message_code: string (nullable = true)\n",
      " |-- message_text: string (nullable = true)\n",
      " |-- weather_lat: float (nullable = true)\n",
      " |-- weather_lon: float (nullable = true)\n",
      " |-- weather_city_id: integer (nullable = true)\n",
      " |-- weather_city_name: string (nullable = true)\n",
      " |-- temp: float (nullable = true)\n",
      " |-- pressure: integer (nullable = true)\n",
      " |-- humidity: integer (nullable = true)\n",
      " |-- wind_speed: integer (nullable = true)\n",
      " |-- wind_deg: integer (nullable = true)\n",
      " |-- rain_3h: float (nullable = true)\n",
      " |-- clouds_all: integer (nullable = true)\n",
      " |-- weather_id: integer (nullable = true)\n",
      " |-- weather_main: string (nullable = true)\n",
      " |-- weather_description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing the schema \n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+-------+----+----------+------+----------------+------------+--------------+-----------------+-----------+-------+-------+--------+----------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+------+--------+--------+----------+--------+-------+----------+----------+------------+-------------------+\n",
      "|year|  month|day|weekday|hour|atm_status|atm_id|atm_manufacturer|atm_location|atm_streetname|atm_street_number|atm_zipcode|atm_lat|atm_lon|currency| card_type|transaction_amount|   service|message_code|message_text|weather_lat|weather_lon|weather_city_id|weather_city_name|  temp|pressure|humidity|wind_speed|wind_deg|rain_3h|clouds_all|weather_id|weather_main|weather_description|\n",
      "+----+-------+---+-------+----+----------+------+----------------+------------+--------------+-----------------+-----------+-------+-------+--------+----------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+------+--------+--------+----------+--------+-------+----------+----------+------------+-------------------+\n",
      "|2017|January|  1| Sunday|   0|    Active|     1|             NCR|    NÃ¦stved|   Farimagsvej|                8|       4700| 55.233| 11.763|     DKK|MasterCard|              5643|Withdrawal|        null|        null|      55.23|     11.761|        2616038|         Naestved|281.15|    1014|      87|         7|     260|  0.215|        92|       500|        Rain|         light rain|\n",
      "+----+-------+---+-------+----+----------+------+----------------+------------+--------------+-----------------+-----------+-------+-------+--------+----------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+------+--------+--------+----------+--------+-------+----------+----------+------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showing the elements of the dataframe\n",
    "df.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468572"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of records which is matching with sqoop job record count.\n",
    "df.select(\"*\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year',\n",
       " 'month',\n",
       " 'day',\n",
       " 'weekday',\n",
       " 'hour',\n",
       " 'atm_status',\n",
       " 'atm_id',\n",
       " 'atm_manufacturer',\n",
       " 'atm_location',\n",
       " 'atm_streetname',\n",
       " 'atm_street_number',\n",
       " 'atm_zipcode',\n",
       " 'atm_lat',\n",
       " 'atm_lon',\n",
       " 'currency',\n",
       " 'card_type',\n",
       " 'transaction_amount',\n",
       " 'service',\n",
       " 'message_code',\n",
       " 'message_text',\n",
       " 'weather_lat',\n",
       " 'weather_lon',\n",
       " 'weather_city_id',\n",
       " 'weather_city_name',\n",
       " 'temp',\n",
       " 'pressure',\n",
       " 'humidity',\n",
       " 'wind_speed',\n",
       " 'wind_deg',\n",
       " 'rain_3h',\n",
       " 'clouds_all',\n",
       " 'weather_id',\n",
       " 'weather_main',\n",
       " 'weather_description']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe which shows all the column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## 1. Location Dimension  ###############\n",
    "# create dataframe for location_dimension which includes atm_location, atm_streetname, atm_street_number, atm_zipcode,\n",
    "# atm_lat and atm_lon columns. Use alias method to convert original column name with given schema name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-------------+-------+------+------+\n",
      "|      location|          streetname|street_number|zipcode|   lat|   lon|\n",
      "+--------------+--------------------+-------------+-------+------+------+\n",
      "|      NÃ¦stved|         Farimagsvej|            8|   4700|55.233|11.763|\n",
      "|      Vejgaard|          Hadsundvej|           20|   9000|57.043|  9.95|\n",
      "|      Vejgaard|          Hadsundvej|           20|   9000|57.043|  9.95|\n",
      "|         Ikast|     RÃ¥dhusstrÃ¦det|           12|   7430|56.139| 9.154|\n",
      "|    Svogerslev|          BrÃ¸nsager|            1|   4000|55.634|12.018|\n",
      "|          Nibe|              Torvet|            1|   9240|56.983| 9.639|\n",
      "|    Fredericia|      SjÃ¦llandsgade|           33|   7000|55.564| 9.757|\n",
      "|     Hjallerup|   Hjallerup Centret|           18|   9320|57.168|10.148|\n",
      "|     GlyngÃ¸re|           FÃ¦rgevej|            1|   7870|56.762| 8.867|\n",
      "|       Hadsund|           Storegade|           12|   9560|56.716|10.114|\n",
      "|  NÃ¸rresundby|              Torvet|            6|   9400|57.059| 9.922|\n",
      "|     Sauersvej|Fridtjof Nansens Vej|            2|   9210|57.023|  9.94|\n",
      "|      Vejgaard|          Hadsundvej|           20|   9000|57.043|  9.95|\n",
      "|Ã˜sterÃ¥  Duus|            Ã˜sterÃ¥|           12|   9000|57.049| 9.922|\n",
      "|         SÃ¦by|          Vestergade|            3|   9300|57.334|10.515|\n",
      "|      HÃ¸rning|          NÃ¸rrealle|           12|   8362|56.086|10.037|\n",
      "|        Vestre|           Kastetvej|           36|   9000|57.053| 9.905|\n",
      "|Ã˜sterÃ¥  Duus|            Ã˜sterÃ¥|           12|   9000|57.049| 9.922|\n",
      "|         Skive|            Adelgade|            8|   7800|56.567| 9.027|\n",
      "|       Randers|          Ã˜stervold|           16|   8900|56.462|10.038|\n",
      "+--------------+--------------------+-------------+-------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([df.atm_location.alias(\"location\"), df.atm_streetname.alias(\"streetname\"),\n",
    "                  df.atm_street_number.alias(\"street_number\"), df.atm_zipcode.alias(\"zipcode\"), df.atm_lat.alias(\"lat\"),\n",
    "                  df.atm_lon.alias(\"lon\")]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the count of location_dimension table and as per below got the same count which is mentioned in the validation doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loc=df.select([ df.atm_location.alias(\"location\"), df.atm_streetname.alias(\"streetname\"),\n",
    "                  df.atm_street_number.alias(\"street_number\"), df.atm_zipcode.alias(\"zipcode\"), df.atm_lat.alias(\"lat\"),\n",
    "                  df.atm_lon.alias(\"lon\")]).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count for the Location Dimension using count() and distinct() method.\n",
    "df_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_location = df.select([df.atm_location.alias(\"location\"), df.atm_streetname.alias(\"streetname\"),\n",
    "                  df.atm_street_number.alias(\"street_number\"), df.atm_zipcode.alias(\"zipcode\"), df.atm_lat.alias(\"lat\"),\n",
    "                 df.atm_lon.alias(\"lon\")]).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------+-------+------+------+\n",
      "|            location|          streetname|street_number|zipcode|   lat|   lon|\n",
      "+--------------------+--------------------+-------------+-------+------+------+\n",
      "|      NykÃ¸bing Mors|         Kirketorvet|            1|   7900|56.795|  8.86|\n",
      "|Aalborg Storcente...|            Hobrovej|          452|   9200|57.005| 9.876|\n",
      "|            Hasseris|         Hasserisvej|          113|   9000|57.044| 9.898|\n",
      "|HillerÃ¸d IdrÃ¦ts...|          Milnersvej|           39|   3400|55.921|12.299|\n",
      "|         Bispensgade|         Bispensgade|           35|   9800|57.453| 9.996|\n",
      "|Intern  BrÃ¸nderslev|              Algade|            4|   9700|57.269| 9.945|\n",
      "|  Intern  KÃ¸benhavn|      RÃ¥dhuspladsen|           75|   1550|55.676|12.571|\n",
      "|               KÃ¸ge|        SÃ¸ndre Alle|            1|   4600|55.454|12.181|\n",
      "|              Odense|          FÃ¦lledvej|            3|   5000|55.394| 10.37|\n",
      "|            Slagelse|     Mariendals Alle|           29|   4200|55.398|11.342|\n",
      "|      Bryggen  Vejle|      SÃ¸nderbrogade|            2|   7100|55.705| 9.532|\n",
      "|           SÃ¦by Syd|Trafikcenter SÃ¦b...|            1|   9300|57.313| 10.45|\n",
      "|            Slagelse|    Mariendals AllÃ¨|           29|   4200|55.398|11.342|\n",
      "|            Roskilde|      KÃ¸benhavnsvej|           65|   4000|55.642|12.106|\n",
      "|         Brugsen ANS|    SÃ¸ndermarksgade|           14|   8643|56.306| 9.594|\n",
      "|NykÃ¸bing Mors Lobby|         Kirketorvet|            1|   7900|56.795|  8.86|\n",
      "|            HÃ¸jslev|        Ã˜sterrisvej|            2|   7840|56.551|  9.11|\n",
      "|             AalbÃ¦k|          Centralvej|            5|   9982|57.593|10.412|\n",
      "|              Viborg|           Toldboden|            3|   8800|56.448| 9.401|\n",
      "|      Intern HolbÃ¦k|         Slotsvolden|            7|   4300|55.718|11.704|\n",
      "+--------------------+--------------------+-------------+-------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_location.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[location: string, streetname: string, street_number: int, zipcode: int, lat: float, lon: float]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Nyk\\xc3\\xb8bing Mors',\n",
       "  u'Kirketorvet',\n",
       "  1,\n",
       "  7900,\n",
       "  56.79499816894531,\n",
       "  8.859999656677246,\n",
       "  1],\n",
       " [u'Aalborg Storcenter indg. D',\n",
       "  u'Hobrovej',\n",
       "  452,\n",
       "  9200,\n",
       "  57.005001068115234,\n",
       "  9.87600040435791,\n",
       "  2],\n",
       " [u'Hasseris',\n",
       "  u'Hasserisvej',\n",
       "  113,\n",
       "  9000,\n",
       "  57.04399871826172,\n",
       "  9.89799976348877,\n",
       "  3],\n",
       " [u'Hiller\\xc3\\xb8d Idr\\xc3\\xa6tscenter',\n",
       "  u'Milnersvej',\n",
       "  39,\n",
       "  3400,\n",
       "  55.92100143432617,\n",
       "  12.298999786376953,\n",
       "  4]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Using rdd method zipWithIndex() with lambda function to append the rdd with element indices.\n",
    "df_location.rdd.zipWithIndex().map(lambda(row, rowId): (list(row)+[rowId+1])).take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_rdd=df_location.rdd.zipWithIndex().map(lambda(row, rowId): (list(row)+[rowId+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding new column location_id which is primary key using lit() method and assign value to the column.\n",
    "from pyspark.sql.functions import lit\n",
    "loc_dim=df_location.withColumn(\"location_id\", lit(\"1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-------------+-------+------+------+-----------+\n",
      "|            location| streetname|street_number|zipcode|   lat|   lon|location_id|\n",
      "+--------------------+-----------+-------------+-------+------+------+-----------+\n",
      "|      NykÃ¸bing Mors|Kirketorvet|            1|   7900|56.795|  8.86|          1|\n",
      "|Aalborg Storcente...|   Hobrovej|          452|   9200|57.005| 9.876|          1|\n",
      "|            Hasseris|Hasserisvej|          113|   9000|57.044| 9.898|          1|\n",
      "|HillerÃ¸d IdrÃ¦ts...| Milnersvej|           39|   3400|55.921|12.299|          1|\n",
      "|         Bispensgade|Bispensgade|           35|   9800|57.453| 9.996|          1|\n",
      "+--------------------+-----------+-------------+-------+------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loc_dim.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe using RDD and assinging unique integer value to the location_id column.\n",
    "df_loc_dim=spark.createDataFrame(loc_rdd,schema=loc_dim.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-------------+-------+------+------+-----------+\n",
      "|            location| streetname|street_number|zipcode|   lat|   lon|location_id|\n",
      "+--------------------+-----------+-------------+-------+------+------+-----------+\n",
      "|      NykÃ¸bing Mors|Kirketorvet|            1|   7900|56.795|  8.86|          1|\n",
      "|Aalborg Storcente...|   Hobrovej|          452|   9200|57.005| 9.876|          2|\n",
      "|            Hasseris|Hasserisvej|          113|   9000|57.044| 9.898|          3|\n",
      "|HillerÃ¸d IdrÃ¦ts...| Milnersvej|           39|   3400|55.921|12.299|          4|\n",
      "|         Bispensgade|Bispensgade|           35|   9800|57.453| 9.996|          5|\n",
      "+--------------------+-----------+-------------+-------+------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_loc_dim.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_loc_dim.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## 2. ATM Dimesnion #################\n",
    "## create ATM_Dimension dataframe with columns atm_id, atm_manufacturer, atm_location, atm_streetname, atm_street_number,\n",
    "## atm_zipcode, atm_lat, atm_lon and use proper naming with alias method as per schema provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atm = df.select([df.atm_id.alias(\"atm_number\"), df.atm_manufacturer, df.atm_location.alias(\"location\"),\n",
    "                  df.atm_streetname.alias(\"streetname\"), df.atm_street_number.alias(\"street_number\"), \n",
    "                  df.atm_zipcode.alias(\"zipcode\"), df.atm_lat.alias(\"lat\"), df.atm_lon.alias(\"lon\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468572"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_atm.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using distinct function get the count of ATM_DIMENSION table\n",
    "df_atm_new = df_atm.distinct().join(df_loc_dim, on=[\"lat\",\"lon\"],\n",
    "                                    how='left').select([\"atm_number\",\"atm_manufacturer\",\"location_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count is matching with validation documents.\n",
    "df_atm_new.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-----------+\n",
      "|atm_number|atm_manufacturer|location_id|\n",
      "+----------+----------------+-----------+\n",
      "|        18| Diebold Nixdorf|         19|\n",
      "|       101|             NCR|         11|\n",
      "|         9| Diebold Nixdorf|         36|\n",
      "|        64|             NCR|         25|\n",
      "|        59| Diebold Nixdorf|          1|\n",
      "|        59| Diebold Nixdorf|         16|\n",
      "|        30|             NCR|          1|\n",
      "|        30|             NCR|         16|\n",
      "|        12|             NCR|         35|\n",
      "|        12|             NCR|         44|\n",
      "|        12|             NCR|        108|\n",
      "|       104|             NCR|         35|\n",
      "|       104|             NCR|         44|\n",
      "|       104|             NCR|        108|\n",
      "|        21|             NCR|         35|\n",
      "|        21|             NCR|         44|\n",
      "|        21|             NCR|        108|\n",
      "|        39|             NCR|         23|\n",
      "|        55|             NCR|         52|\n",
      "|        22|             NCR|         14|\n",
      "+----------+----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_atm_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using rdd method zipWithIndex() with lambda function to append the rdd with element indices.\n",
    "atm_rdd = df_atm_new.rdd.zipWithIndex().map(lambda (row, rowId):(list(row)+[rowId+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-----------+------+\n",
      "|atm_number|atm_manufacturer|location_id|atm_id|\n",
      "+----------+----------------+-----------+------+\n",
      "|        18| Diebold Nixdorf|         19|     1|\n",
      "|       101|             NCR|         11|     1|\n",
      "|         9| Diebold Nixdorf|         36|     1|\n",
      "|        64|             NCR|         25|     1|\n",
      "|        59| Diebold Nixdorf|          1|     1|\n",
      "|        59| Diebold Nixdorf|         16|     1|\n",
      "|        30|             NCR|          1|     1|\n",
      "|        30|             NCR|         16|     1|\n",
      "|        12|             NCR|         35|     1|\n",
      "|        12|             NCR|         44|     1|\n",
      "|        12|             NCR|        108|     1|\n",
      "|       104|             NCR|         35|     1|\n",
      "|       104|             NCR|         44|     1|\n",
      "|       104|             NCR|        108|     1|\n",
      "|        21|             NCR|         35|     1|\n",
      "|        21|             NCR|         44|     1|\n",
      "|        21|             NCR|        108|     1|\n",
      "|        39|             NCR|         23|     1|\n",
      "|        55|             NCR|         52|     1|\n",
      "|        22|             NCR|         14|     1|\n",
      "+----------+----------------+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adding new column atm_id which is primary key using lit() method and assign value to the column.\n",
    "atm_dim=df_atm_new.withColumn(\"atm_id\",lit(1))\n",
    "atm_dim.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe using RDD and assinging unique integer value to the location_id column.\n",
    "df_atm_dim = spark.createDataFrame(atm_rdd, schema=atm_dim.schema).select([\"atm_id\",\"atm_number\",\"atm_manufacturer\",\n",
    "                                                                           \"location_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count of atm_dim dataframe\n",
    "df_atm_dim.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------------+-----------+\n",
      "|atm_id|atm_number|atm_manufacturer|location_id|\n",
      "+------+----------+----------------+-----------+\n",
      "|     1|        18| Diebold Nixdorf|         19|\n",
      "|     2|       101|             NCR|         11|\n",
      "|     3|         9| Diebold Nixdorf|         36|\n",
      "|     4|        64|             NCR|         25|\n",
      "|     5|        59| Diebold Nixdorf|          1|\n",
      "|     6|        59| Diebold Nixdorf|         16|\n",
      "|     7|        30|             NCR|          1|\n",
      "|     8|        30|             NCR|         16|\n",
      "|     9|        12|             NCR|         35|\n",
      "|    10|        12|             NCR|         44|\n",
      "|    11|        12|             NCR|        108|\n",
      "|    12|       104|             NCR|         35|\n",
      "|    13|       104|             NCR|         44|\n",
      "|    14|       104|             NCR|        108|\n",
      "|    15|        21|             NCR|         35|\n",
      "|    16|        21|             NCR|         44|\n",
      "|    17|        21|             NCR|        108|\n",
      "|    18|        39|             NCR|         23|\n",
      "|    19|        55|             NCR|         52|\n",
      "|    20|        22|             NCR|         14|\n",
      "+------+----------+----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_atm_dim.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### 3. Date Dimension ##########\n",
    "## creating Date_Dimension dataframe with columns year, month, day, hour and weekday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date = df.select([df.year, df.month, df.day, df.hour, df.weekday])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying distinct on date_dim dataframe and change/convert the value of year, month, day and hour using unixTimestamp()\n",
    "## method and concatenating the value in new column full_date_time. UnixTimeStamp is used to convert the string type data \n",
    "## values into datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+----+--------+-------------------+\n",
      "|year|  month|day|hour| weekday|     full_date_time|\n",
      "+----+-------+---+----+--------+-------------------+\n",
      "|2017|January|  1|   9|  Sunday|2017-01-01 09:00:00|\n",
      "|2017|January|  3|   5| Tuesday|2017-01-03 05:00:00|\n",
      "|2017|January|  8|  19|  Sunday|2017-01-08 19:00:00|\n",
      "|2017|January| 21|   3|Saturday|2017-01-21 03:00:00|\n",
      "|2017|January| 23|  21|  Monday|2017-01-23 21:00:00|\n",
      "+----+-------+---+----+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df_date_new=df_date.distinct().withColumn(\"full_date_time\", from_unixtime(unix_timestamp(concat(df_date.year.cast(StringType()),\n",
    "            df_date.month.cast(StringType()), lpad(df.day.cast(StringType()),2,'0'),\n",
    "            lpad(df.hour.cast(StringType()),2,'0')),'yyyyMMMMMddHH'),'YYYY-MM-dd HH:mm:ss'))\n",
    "df_date_new.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8685"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting exact count of date_dim dataframe as per validation document.\n",
    "df_date_new.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using rdd method zipWithIndex() with lambda function to append the rdd with element indices.\n",
    "date_rdd=df_date_new.rdd.zipWithIndex().map(lambda (row, rowId):(list(row)+[rowId+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+----+--------+-------------------+-------+\n",
      "|year|  month|day|hour| weekday|     full_date_time|date_id|\n",
      "+----+-------+---+----+--------+-------------------+-------+\n",
      "|2017|January|  1|   9|  Sunday|2017-01-01 09:00:00|      1|\n",
      "|2017|January|  3|   5| Tuesday|2017-01-03 05:00:00|      1|\n",
      "|2017|January|  8|  19|  Sunday|2017-01-08 19:00:00|      1|\n",
      "|2017|January| 21|   3|Saturday|2017-01-21 03:00:00|      1|\n",
      "|2017|January| 23|  21|  Monday|2017-01-23 21:00:00|      1|\n",
      "+----+-------+---+----+--------+-------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adding new column date_id which is primary key using lit() method and assign value to the column.\n",
    "date_dim=df_date_new.withColumn(\"date_id\",lit(1))\n",
    "date_dim.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----+--------+---+----+--------+\n",
      "|date_id|     full_date_time|year|   month|day|hour| weekday|\n",
      "+-------+-------------------+----+--------+---+----+--------+\n",
      "|      1|2017-01-01 09:00:00|2017| January|  1|   9|  Sunday|\n",
      "|      2|2017-01-03 05:00:00|2017| January|  3|   5| Tuesday|\n",
      "|      3|2017-01-08 19:00:00|2017| January|  8|  19|  Sunday|\n",
      "|      4|2017-01-21 03:00:00|2017| January| 21|   3|Saturday|\n",
      "|      5|2017-01-23 21:00:00|2017| January| 23|  21|  Monday|\n",
      "|      6|2017-02-02 19:00:00|2017|February|  2|  19|Thursday|\n",
      "|      7|2017-02-05 16:00:00|2017|February|  5|  16|  Sunday|\n",
      "|      8|2017-02-21 15:00:00|2017|February| 21|  15| Tuesday|\n",
      "|      9|2017-03-02 08:00:00|2017|   March|  2|   8|Thursday|\n",
      "|     10|2017-04-02 02:00:00|2017|   April|  2|   2|  Sunday|\n",
      "|     11|2017-04-06 08:00:00|2017|   April|  6|   8|Thursday|\n",
      "|     12|2017-04-30 10:00:00|2017|   April| 30|  10|  Sunday|\n",
      "|     13|2017-05-02 02:00:00|2017|     May|  2|   2| Tuesday|\n",
      "|     14|2017-05-20 16:00:00|2017|     May| 20|  16|Saturday|\n",
      "|     15|2017-05-21 19:00:00|2017|     May| 21|  19|  Sunday|\n",
      "|     16|2017-06-27 00:00:00|2017|    June| 27|   0| Tuesday|\n",
      "|     17|2017-07-18 09:00:00|2017|    July| 18|   9| Tuesday|\n",
      "|     18|2017-07-18 22:00:00|2017|    July| 18|  22| Tuesday|\n",
      "|     19|2017-07-20 00:00:00|2017|    July| 20|   0|Thursday|\n",
      "|     20|2017-07-21 19:00:00|2017|    July| 21|  19|  Friday|\n",
      "+-------+-------------------+----+--------+---+----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating final dataframe for date_dim.\n",
    "df_date_dim=spark.createDataFrame(date_rdd, schema=date_dim.schema).select([\"date_id\",\"full_date_time\",\"year\",\n",
    "                                \"month\",\"day\",\"hour\",\"weekday\"])\n",
    "df_date_dim.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8685"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_date_dim.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## 4. Card Type Dimension ##############\n",
    "# creating dataframe with column card_type for card_type_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           card_type|\n",
      "+--------------------+\n",
      "|     Dankort - on-us|\n",
      "|              CIRRUS|\n",
      "|                VISA|\n",
      "|  Mastercard - on-us|\n",
      "|             Maestro|\n",
      "|Visa Dankort - on-us|\n",
      "|        Visa Dankort|\n",
      "|            VisaPlus|\n",
      "|           HÃ¦vekort|\n",
      "|          MasterCard|\n",
      "|             Dankort|\n",
      "|   HÃ¦vekort - on-us|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_card = df.select([df.card_type]).distinct()\n",
    "df_card.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count is matching as per validation doc.\n",
    "df_card.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using rdd method zipWithIndex() with lambda function to append the rdd with element indices.\n",
    "card_rdd = df_card.rdd.zipWithIndex().map(lambda (row, rowId):(list(row)+[rowId+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+\n",
      "|         card_type|card_type_id|\n",
      "+------------------+------------+\n",
      "|   Dankort - on-us|           1|\n",
      "|            CIRRUS|           1|\n",
      "|              VISA|           1|\n",
      "|Mastercard - on-us|           1|\n",
      "|           Maestro|           1|\n",
      "+------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adding new column card_type_id which is primary key using lit() method and assign value to the column.\n",
    "card_dim=df_card.withColumn(\"card_type_id\",lit(1))\n",
    "card_dim.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|card_type_id|           card_type|\n",
      "+------------+--------------------+\n",
      "|           1|     Dankort - on-us|\n",
      "|           2|              CIRRUS|\n",
      "|           3|                VISA|\n",
      "|           4|  Mastercard - on-us|\n",
      "|           5|             Maestro|\n",
      "|           6|Visa Dankort - on-us|\n",
      "|           7|        Visa Dankort|\n",
      "|           8|            VisaPlus|\n",
      "|           9|           HÃ¦vekort|\n",
      "|          10|          MasterCard|\n",
      "|          11|             Dankort|\n",
      "|          12|   HÃ¦vekort - on-us|\n",
      "+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating final dataframe for card_type_dimension \n",
    "df_card_type_dim = spark.createDataFrame(card_rdd, schema=card_dim.schema).select([\"card_type_id\", \"card_type\"])\n",
    "df_card_type_dim.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_card_type_dim.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Fact Table --> FACT_ATM_TRANS  #################\n",
    "#### for creating fact table we need to merge all the above created dimension table dataframe one by one on the provided schema\n",
    "#### information. below mentioned the list of columns which we need to use while creating fact_table mapping.\n",
    "#### 1.location_dim -> location, streetname, street_number,zipcode,lat,lon\n",
    "#### 2.atm_dim -> atm_id,atm_number\n",
    "#### 3.date_dim -> year, month, day, hour,weekday\n",
    "#### 4.card_dim -> card_type\n",
    "\n",
    "####### Alias Creation- first creating alias of all the above created dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.alias('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loc_dim=df_loc_dim.alias(\"df_loc_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atm_dim=df_atm_dim.alias(\"df_atm_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_dim=df_date_dim.alias(\"df_date_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_card_type_dim=df_card_type_dim.alias(\"df_card_type_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Joining above created alias dataframe with created dim dataframe based on schema provided. and dropping those columns from \n",
    "## the defined dataframe so that those columns will not present in fact_table. \n",
    "\n",
    "## join with date_dim dataframe using left outer join with original datafram which we created after schema mapping and \n",
    "## stored the result set in df_stg1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stg1=df.join(df_date_dim, on=[\"year\", \"month\", \"day\", \"hour\",\"weekday\"], how=\"left\").select(\"df.*\",\"df_date_dim.date_id\").drop(\n",
    "                                *[\"year\", \"month\", \"day\", \"hour\",\"weekday\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------------+--------------------+--------------+-----------------+-----------+-------+-------+--------+------------------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+-------+--------+--------+----------+--------+-------+----------+----------+------------+-------------------+-------+\n",
      "|atm_status|atm_id|atm_manufacturer|        atm_location|atm_streetname|atm_street_number|atm_zipcode|atm_lat|atm_lon|currency|         card_type|transaction_amount|   service|message_code|message_text|weather_lat|weather_lon|weather_city_id|weather_city_name|   temp|pressure|humidity|wind_speed|wind_deg|rain_3h|clouds_all|weather_id|weather_main|weather_description|date_id|\n",
      "+----------+------+----------------+--------------------+--------------+-----------------+-----------+-------+-------+--------+------------------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+-------+--------+--------+----------+--------+-------+----------+----------+------------+-------------------+-------+\n",
      "|  Inactive|    52|             NCR|              FarsÃ¸|        Torvet|                8|       9640| 56.771|   9.34|     DKK|Mastercard - on-us|              5244|Withdrawal|        null|        null|     56.773|      9.339|        2622310|            Farso|286.321|    1029|      74|         7|     244|    0.0|         0|       800|       Clear|       Sky is Clear|     61|\n",
      "|    Active|    26|             NCR|             HolbÃ¦k|   Slotsvolden|                7|       4300| 55.718| 11.704|     DKK|Mastercard - on-us|              9317|Withdrawal|        null|        null|       55.7|       11.7|        2620141|  HolbÃ¦k Kommune| 287.15|    1022|      54|         4|     250|    0.0|         0|       800|       Clear|       Sky is Clear|     61|\n",
      "|    Active|    87|             NCR|Aalborg Storcente...|      Hobrovej|              452|       9200| 57.005|  9.876|     DKK|        MasterCard|               850|Withdrawal|        null|        null|     56.972|      9.848|        2612021|        Svenstrup|286.321|    1029|      74|         7|     244|    0.0|         0|       800|       Clear|       Sky is Clear|     61|\n",
      "|    Active|    78| Diebold Nixdorf|              Nyborg|    Vestergade|               35|       5800| 55.318| 10.781|     DKK|              VISA|              6299|Withdrawal|        null|        null|      55.06|     10.607|        2612045|        Svendborg| 288.65|    1021|      55|         6|     210|    0.0|         0|       800|       Clear|       Sky is Clear|     61|\n",
      "|    Active|    46| Diebold Nixdorf|          HelsingÃ¸r|Sct. Olai Gade|               39|       3000| 56.036| 12.612|     DKK|Mastercard - on-us|              8947|Withdrawal|        null|        null|     56.036|     12.614|        2620473|        Helsingor| 286.43|    1021|      62|         4|     280|    0.0|         0|       701|        Mist|               mist|     61|\n",
      "+----------+------+----------------+--------------------+--------------+-----------------+-----------+-------+-------+--------+------------------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+-------+--------+--------+----------+--------+-------+----------+----------+------------+-------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stg1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stg1=df_stg1.alias(\"df_stg1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------------+------------+--------------+-----------------+-----------+-------+-------+--------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+-------+--------+--------+----------+--------+-------+----------+----------+------------+-------------------+-------+------------+\n",
      "|atm_status|atm_id|atm_manufacturer|atm_location|atm_streetname|atm_street_number|atm_zipcode|atm_lat|atm_lon|currency|transaction_amount|   service|message_code|message_text|weather_lat|weather_lon|weather_city_id|weather_city_name|   temp|pressure|humidity|wind_speed|wind_deg|rain_3h|clouds_all|weather_id|weather_main|weather_description|date_id|card_type_id|\n",
      "+----------+------+----------------+------------+--------------+-----------------+-----------+-------+-------+--------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+-------+--------+--------+----------+--------+-------+----------+----------+------------+-------------------+-------+------------+\n",
      "|    Active|    62| Diebold Nixdorf|    Terndrup|      Bymidten|                2|       9575| 56.815| 10.057|     DKK|               936|Withdrawal|        null|        null|     56.715|     10.117|        2620952|          Hadsund|286.321|    1029|      74|         7|     244|    0.0|         0|       800|       Clear|       Sky is Clear|     61|           1|\n",
      "|    Active|    69|             NCR|       Taars|      Bredgade|               91|       9830| 57.385| 10.116|     DKK|              5612|Withdrawal|        null|        null|     57.383|     10.117|        2611852|             Tars|286.321|    1029|      74|         7|     244|    0.0|         0|       800|       Clear|       Sky is Clear|     61|           1|\n",
      "+----------+------+----------------+------------+--------------+-----------------+-----------+-------+-------+--------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+-------+--------+--------+----------+--------+-------+----------+----------+------------+-------------------+-------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Now joining (left join) df_stg1 dataframe with df_card_type_dim and dropping card_type column.\n",
    "df_stg2=df_stg1.join(df_card_type_dim, on=[\"card_type\"], how=\"left\").select(\"df_stg1.*\",\"df_card_type_dim.card_type_id\").drop(\n",
    "                                        *[\"card_type\"])\n",
    "df_stg2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stg2 = df_stg2.alias(\"df_stg2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## renaming few columns with withColumnRenamed() method and then joining df_stg2 with loc_dim dataframe.\n",
    "df_stg3 = df_stg2.withColumnRenamed(\"atm_location\",\"location\").withColumnRenamed(\"atm_lat\",\"lat\").withColumnRenamed(\"atm_lon\",\n",
    "                    \"lon\").withColumnRenamed(\"atm_streetname\",\"streetname\").withColumnRenamed(\"atm_street_number\",\n",
    "                    \"street_number\").withColumnRenamed(\"atm_zipcode\",\"zipcode\").join(df_loc_dim, on=[\"location\", \"streetname\", \"street_number\",\"zipcode\",\"lat\",\"lon\"], how=\"left\").select(\"df_stg2.*\",\"df_loc_dim.location_id\").drop(\n",
    "                    *[\"location\", \"streetname\", \"street_number\",\"zipcode\",\"lat\",\"lon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------------+--------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+------+--------+--------+----------+--------+-------+----------+----------+------------+-------------------+-------+------------+-----------+\n",
      "|atm_status|atm_id|atm_manufacturer|currency|transaction_amount|   service|message_code|message_text|weather_lat|weather_lon|weather_city_id|weather_city_name|  temp|pressure|humidity|wind_speed|wind_deg|rain_3h|clouds_all|weather_id|weather_main|weather_description|date_id|card_type_id|location_id|\n",
      "+----------+------+----------------+--------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+------+--------+--------+----------+--------+-------+----------+----------+------------+-------------------+-------+------------+-----------+\n",
      "|  Inactive|    89|             NCR|     DKK|               463|Withdrawal|        null|        null|     57.441|     10.537|        2621927|    Frederikshavn|272.15|    1032|      58|         9|      80|    0.0|        75|       600|        Snow|         light snow|   3477|           1|         39|\n",
      "|  Inactive|    89|             NCR|     DKK|              5086|Withdrawal|        null|        null|     57.441|     10.537|        2621927|    Frederikshavn|272.15|    1032|      58|         9|      80|    0.0|        75|       600|        Snow|         light snow|   3477|           1|         39|\n",
      "+----------+------+----------------+--------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+------+--------+--------+----------+--------+-------+----------+----------+------------+-------------------+-------+------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stg3.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stg3=df_stg3.alias(\"df_stg3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+------+--------+--------+----------+--------+-------+----------+----------+------------+-------------------+-------+------------+------+\n",
      "|location_id|atm_status|currency|transaction_amount|   service|message_code|message_text|weather_lat|weather_lon|weather_city_id|weather_city_name|  temp|pressure|humidity|wind_speed|wind_deg|rain_3h|clouds_all|weather_id|weather_main|weather_description|date_id|card_type_id|atm_id|\n",
      "+-----------+----------+--------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+------+--------+--------+----------+--------+-------+----------+----------+------------+-------------------+-------+------------+------+\n",
      "|         11|    Active|     DKK|              2613|Withdrawal|        null|        null|     55.709|      9.536|        2610613|            Vejle|276.35|    1023|      93|         3|     220|    0.0|        48|       701|        Mist|               mist|   6975|           1|     2|\n",
      "|         11|    Active|     DKK|              7129|Withdrawal|        null|        null|     55.709|      9.536|        2610613|            Vejle|276.35|    1023|      93|         3|     220|    0.0|        48|       701|        Mist|               mist|   6975|           1|     2|\n",
      "+-----------+----------+--------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+------+--------+--------+----------+--------+-------+----------+----------+------------+-------------------+-------+------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# last we are joining with atm_dim dataframe and dropping unused columns.\n",
    "df_stg4=df_stg3.withColumnRenamed(\"atm_id\",\"atm_number\").join(df_atm_dim, on=['atm_number','atm_manufacturer','location_id'],\n",
    "                    how=\"left\").select(\"df_stg3.*\",\"df_atm_dim.atm_id\").drop(*['atm_number','atm_manufacturer'])\n",
    "\n",
    "df_stg4.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468572"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stg4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using rdd method zipWithIndex() with lambda function to append the rdd with element indices.\n",
    "fact_rdd=df_stg4.rdd.zipWithIndex().map(lambda (row, rowId):(list(row)+[rowId+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding primary key column of fact_table dataframe with lit() method.\n",
    "fact_atm=df_stg4.withColumn(\"trans_id\", lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+-------+------------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+--------------------+\n",
      "|trans_id|atm_id|location_id|date_id|card_type_id|atm_status|currency|   service|transaction_amount|message_code|message_text|rain_3h|clouds_all|weather_id|weather_main| weather_description|\n",
      "+--------+------+-----------+-------+------------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+--------------------+\n",
      "|       1|     2|         11|   6975|           1|    Active|     DKK|Withdrawal|              2613|        null|        null|    0.0|        48|       701|        Mist|                mist|\n",
      "|       2|     2|         11|   6975|           1|    Active|     DKK|Withdrawal|              7129|        null|        null|    0.0|        48|       701|        Mist|                mist|\n",
      "|       3|     2|         11|   3066|           1|    Active|     DKK|Withdrawal|              1552|        null|        null|    0.0|        92|       500|        Rain|          light rain|\n",
      "|       4|     2|         11|    926|           1|    Active|     DKK|Withdrawal|              5079|        null|        null|    0.0|        88|       520|        Rain|light intensity s...|\n",
      "|       5|     2|         11|   7809|           1|    Active|     DKK|Withdrawal|               806|        null|        null|    0.0|        56|       803|      Clouds|       broken clouds|\n",
      "|       6|     2|         11|   6604|           1|    Active|     DKK|Withdrawal|              7698|        null|        null|    0.0|        90|       520|        Rain|light intensity s...|\n",
      "|       7|     2|         11|   6189|           1|    Active|     DKK|Withdrawal|               992|        null|        null|    0.0|        75|       520|        Rain|light intensity s...|\n",
      "|       8|     2|         11|   6651|           1|    Active|     DKK|Withdrawal|              6315|        null|        null|    0.0|        44|       802|      Clouds|    scattered clouds|\n",
      "|       9|     2|         11|   6832|           1|    Active|     DKK|Withdrawal|              7343|        null|        null|    0.0|        12|       801|      Clouds|          few clouds|\n",
      "|      10|     2|         11|   7493|           1|    Active|     DKK|Withdrawal|              8566|        null|        null|    0.0|         0|       800|       Clear|        Sky is Clear|\n",
      "|      11|     2|         11|   8371|           1|    Active|     DKK|Withdrawal|              9720|        null|        null|    0.0|        40|       520|        Rain|light intensity s...|\n",
      "|      12|     2|         11|   5180|           1|    Active|     DKK|Withdrawal|              1970|        null|        null|    0.0|        68|       803|      Clouds|       broken clouds|\n",
      "|      13|     2|         11|   8299|           1|    Active|     DKK|Withdrawal|              7524|        null|        null|    0.0|        75|       300|     Drizzle|light intensity d...|\n",
      "|      14|     2|         11|   3915|           1|    Active|     DKK|Withdrawal|              8032|        null|        null|    0.0|        92|       701|        Mist|                mist|\n",
      "|      15|     2|         11|     62|           1|    Active|     DKK|Withdrawal|              8355|        null|        null|    0.0|        75|       310|     Drizzle|light intensity d...|\n",
      "|      16|     2|         11|   2494|           1|    Active|     DKK|Withdrawal|               245|        null|        null|    0.0|        44|       802|      Clouds|    scattered clouds|\n",
      "|      17|     2|         11|   7655|           1|    Active|     DKK|Withdrawal|              3765|        null|        null|    0.0|        64|       803|      Clouds|       broken clouds|\n",
      "|      18|     2|         11|   2071|           1|    Active|     DKK|Withdrawal|              6285|        null|        null|    0.0|        24|       801|      Clouds|          few clouds|\n",
      "|      19|     2|         11|   2071|           1|    Active|     DKK|Withdrawal|              4983|        null|        null|    0.0|        24|       801|      Clouds|          few clouds|\n",
      "|      20|     2|         11|    838|           1|    Active|     DKK|Withdrawal|              1827|        null|        null|   1.44|        80|       500|        Rain|          light rain|\n",
      "+--------+------+-----------+-------+------------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## final dataframe of fact_table - fact_atm_trans and below showing details.\n",
    "df_fact_atm_trans = spark.createDataFrame(fact_rdd, schema=fact_atm.schema).select([\"trans_id\", \"atm_id\",\"location_id\",\n",
    "                    \"date_id\",\"card_type_id\",\"atm_status\",\"currency\",\"service\",\"transaction_amount\",\"message_code\",\n",
    "                    \"message_text\",\"rain_3h\",\"clouds_all\",\"weather_id\",\"weather_main\",\"weather_description\"])\n",
    "\n",
    "df_fact_atm_trans.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468572"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matched count with validation doc and same count with sqoop import. \n",
    "df_fact_atm_trans.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### LOAD CSV files to S3 bucket s3://etl-project-dsc18/location-dim/  ########\n",
    "## using write.csv() method we are loading all the above dataframe result set into s3 bucket.\n",
    "## coalesce uses existing partitions to minimize the amount of data that's shuffled. repartition creates new partitions \n",
    "## and does a full shuffle. coalesce results in partitions with different amounts of data (sometimes partitions that have\n",
    "## much different sizes) and repartition results in roughly equal sized partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_dim_file = df_loc_dim.coalesce(1).write.csv(\"s3a://etl-project-dsc18/location-dim/\", mode=\"overwrite\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_dim_file = df_atm_dim.coalesce(1).write.csv(\"s3a://etl-project-dsc18/atm-dim/\", mode=\"overwrite\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_dim_file = df_date_dim.coalesce(1).write.csv(\"s3a://etl-project-dsc18/date-dim/\", mode=\"overwrite\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_type_file = df_card_type_dim.coalesce(1).write.csv(\"s3a://etl-project-dsc18/card-type-dim/\", mode=\"overwrite\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_file = df_fact_atm_trans.coalesce(1).write.csv(\"s3a://etl-project-dsc18/fact-atm-trans/\", mode=\"overwrite\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Prepared by - Pritamkumar. PGDDS-C18 ETL PROJECT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
